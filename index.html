<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content='We propose an algorithm that automatically identifies human-interpretable and transferable prompts that can effectively generate desired concepts given only black-box access to T2I models.'>
  <meta property="og:title" content="PRISM: Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation"/>
  <meta property="og:description" content="We propose an algorithm that automatically identifies human-interpretable and transferable prompts that can effectively generate desired concepts given only black-box access to T2I models."/>
  <meta property="og:url" content="https://kellyyutonghe.github.io/prism/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/intro_figure.png" />
  <meta property="og:image:width" content="2048"/>
  <meta property="og:image:height" content="1315"/>


  <!-- <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG"> -->
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image"> -->
  <!-- Keywords for your paper to be indexed by-->
  <!-- <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1"> -->


  <title>PRISM: Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation</title>
  <link rel="icon" type="image/x-icon" href="static/images/prism_beige_circle.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">PRISM: Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://kellyyutonghe.github.io/" target="_blank">Yutong He</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://arobey1.github.io/" target="_blank"> Alexander Robey</a><sup>1,2</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=oyuTmwoAAAAJ&hl=ja" target="_blank">Naoki Murata</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="https://yidingjiang.github.io/" target="_blank">Yiding Jiang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://jnwilliams.github.io/" target="_blank">Joshua Nathaniel Williams</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.georgejpappas.org/" target="_blank">George J. Pappas</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://www.seas.upenn.edu/~hassani/" target="_blank">Hamed Hassani</a><span><sup>2</sup></span>,</span>
              <span class="author-block">
                <a href="https://www.yukimitsufuji.com/" target="_blank">Yuki Mitsufuji</a><sup>3,4</sup>,</span>
              <span class="author-block">
                <a href="https://www.cs.cmu.edu/~rsalakhu/" target="_blank">Ruslan Salakhutdinov</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://zicokolter.com/" target="_blank">J. Zico Kolter</a><sup>1,5</sup>,</span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Carnegie Mellon University, <sup>2</sup>University of Pennsylvania, <sup>3</sup>Sony AI, <sup>4</sup>Sony Group Corporation, <sup>5</sup>Bosch Center for AI<br></span>
              <!-- <span class="eql-cntrb"><small><br><sup>❋</sup>Equal Contribution</small></span> -->
              <!-- <span class="eql-cntrb"><small><br><sup>♨</sup>Internship at Sony AI</small></span> -->
              <span class="eql-cntrb"><br><b>TMLR 2025</b></span>
            </div>
            
                <div class="column has-text-centered">
                  <div class="publication-links">
                       <!-- PDF link -->
                    
                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->
                  
                  <!-- ArXiv abstract Link -->
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2403.19103" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                    <span class="link-block">
                      <a href="static/pdfs/paper.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/KellyYutongHe/prism_demo" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/intro-1.png">
      <h2 class="subtitle has-text-centered">
        Given a set of reference images, our method, PRISM, is capable of creating human-interpretable and accurate prompts for the desired concept that are also transferable to both open-sourced and closed-sourced text-to-image models.
      </h2>
    </div>
  </div>
</section>


<!-- <section class="hero is-small">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
            <img src="static/images/intro_figure.png" width="1920"/>
            <h2 class="subtitle has-text-centered">
              CAC as a plugin to existing methods for localized text-to-image generation. CAC improves upon diverse types of localization (bounding boxes, semantic segmentation maps and localized styles) with different base models (Stable Diffusion and GLIGEN).
            </h2>
          </div>
          <div class="item">
            <img src="static/images/extra_arbitrary.png" height="1920"/>
            <h2 class="subtitle has-text-centered">
              CAC as a plugin to existing methods for localized text-to-image generation. CAC improves upon diverse types of localization (bounding boxes, semantic segmentation maps and localized styles) with different base models (Stable Diffusion and GLIGEN).
            </h2>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End teaser image -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Prompt engineering is effective for controlling the output of text-to-image (T2I) generative models, but it is also laborious due to the need for manually crafted prompts. This challenge has spurred the development of algorithms for automated prompt generation. However, these methods often struggle with transferability across T2I models, require white-box access to the underlying model, and produce non-intuitive prompts. In this work, we introduce PRISM, an algorithm that automatically identifies human-interpretable and transferable prompts that can effectively generate desired concepts given only black-box access to T2I models. Inspired by large language model (LLM) jailbreaking, PRISM leverages the in-context learning ability of LLMs to iteratively refine the candidate prompts distribution for given reference images. Our experiments demonstrate the versatility and effectiveness of PRISM in generating accurate prompts for objects, styles and images across multiple T2I models, including Stable Diffusion, DALL-E, and Midjourney.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Method figure -->
  <!-- <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered is-four-fifths">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Proposed Method</h2>
          <img src="static/images/pipeline.png">
          <h2 class="subtitle has-text-justified">
            <p>
            CAC as a plugin to existing methods for localized text-to-image generation. CAC improves upon diverse types of localization (bounding boxes, semantic segmentation maps and localized styles) with different base models (Stable Diffusion and GLIGEN).
            </p>
          </h2>
        </div>
    </div>
  </div>
</section> -->

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3 has-text-centered">Proposed Method</h2>
      <img src="static/images/method-1.png">
      <h2 class="subtitle has-text-centered">
        An illustration of PRISM. The label “System” indicates the system prompts setups for the VLMs.
      </h2>
    </div>
  </div>
</section>
<!-- End method figure -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <h2 class="title is-3 has-text-centered">Demo</h2>
    <div class="container is-max-desktop">
      <!-- <div id="results-carousel" class="carousel results-carousel"> -->
       <!-- <div class="item"> -->
        <img src="static/images/object-1.png"/>
        <h2 class="subtitle has-text-centered">
          Qualitative results for object oriented personalized T2I generation on DreamBooth dataset.
        </h2>
      </div>
      <!-- <div class="item">
        <img src="static/images/extra_arbitrary2.png"/>
        <h2 class="subtitle has-text-centered">
          CAC is versatile and can be applied to various application scenarios.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/extra_arbitrary3.png"/>
        <h2 class="subtitle has-text-centered">
          Examples of generated images with a variety of different types of user inputs and applications.
        </h2>
      </div> -->
  <!-- </div> -->
</div>
</div>
</section>
<!-- End image carousel -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <!-- <div id="results-carousel" class="carousel results-carousel"> -->
       <!-- <div class="item"> -->
        <img src="static/images/style-1.png"/>
        <h2 class="subtitle has-text-centered">
          Qualitative results for object oriented personalized T2I generation.
        </h2>
      <!-- </div> -->
      <!-- <div class="item">
        <img src="static/images/coco2.png"/>
        <h2 class="subtitle has-text-centered">
          Examples and baseline comparison of generated images based on bounding box information.
        </h2>
      </div> -->
  <!-- </div> -->
</div>
</div>
</section>
<!-- End image carousel -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <!-- <div id="results-carousel" class="carousel results-carousel">
       <div class="item"> -->
        <img src="static/images/img-1.png"/>
        <h2 class="subtitle has-text-centered">
          Image inversion results for different methods on different T2I models.
        </h2>
      </div>
      <!-- <div class="item">
        <img src="static/images/cc5002.png"/>
        <h2 class="subtitle has-text-centered">
          Examples and baseline comparison of compositional generation.
        </h2>
      </div>
  </div> -->
</div>
</div>
</section>
<!-- End image carousel -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <!-- <div id="results-carousel" class="carousel results-carousel">
       <div class="item"> -->
        <img src="static/images/edit-1.png"/>
        <h2 class="subtitle has-text-centered">
          Prompt editing demonstration with Midjourney.
        </h2>
      </div>
      <!-- <div class="item">
        <img src="static/images/cityscapes1.png"/>
        <h2 class="subtitle has-text-centered">
          Examples and baseline comparison of generated images based on semantic segmentation map information.
        </h2>
      </div>
  </div> -->
</div>
</div>
</section>
<!-- End image carousel -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <!-- <div id="results-carousel" class="carousel results-carousel">
       <div class="item"> -->
        <img src="static/images/multiconcept.png"/>
        <h2 class="subtitle has-text-centered">
          Multi-concept generation demonstration with Midjourney.
        </h2>
      </div>
      <!-- <div class="item">
        <img src="static/images/cityscapes1.png"/>
        <h2 class="subtitle has-text-centered">
          Examples and baseline comparison of generated images based on semantic segmentation map information.
        </h2>
      </div>
  </div> -->
</div>
</div>
</section>
<!-- End image carousel -->






<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">

          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->


<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Ablation Study: The Fidelity-Controllability Trade-offs</h2>
      <img src="static/images/ablation_example.png">
      <h2 class="subtitle has-text-centered">
        Comparison of the fidelity-controllability trade-offs with and without CAC.
        With CAC applied, the model is able to reach a sweet spot where the generated images appear more
        realistic while maintaining better consistency with the bounding box information.
      </h2>
    </div>
  </div>
</section> -->




<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>

      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @article{
          he2025automated,
          title={Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation},
          author={Yutong He and Alexander Robey and Naoki Murata and Yiding Jiang and Joshua Nathaniel Williams and George J. Pappas and Hamed Hassani and Yuki Mitsufuji and Ruslan Salakhutdinov and J Zico Kolter},
          journal={Transactions on Machine Learning Research},
          issn={2835-8856},
          year={2025},
          url={https://openreview.net/forum?id=IVYVDN6pJ6},
          note={}
        }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
